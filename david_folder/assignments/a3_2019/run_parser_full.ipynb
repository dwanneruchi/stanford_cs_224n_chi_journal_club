{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /mnt/c/Users/David/OneDrive/shared_repos/stanford_cs_224n_chi_journal_club/nlp/lib/python3.6/site-packages\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import pickle\n",
    "import math\n",
    "import time\n",
    "\n",
    "from torch import nn, optim\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from parser_model import ParserModel\n",
    "from utils.parser_utils import minibatches, load_and_preprocess_data, AverageMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Primary Functions\n",
    "# -----------------\n",
    "def train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005):\n",
    "    \"\"\" Train the neural dependency parser.\n",
    "\n",
    "    @param parser (Parser): Neural Dependency Parser\n",
    "    @param train_data ():\n",
    "    @param dev_data ():\n",
    "    @param output_path (str): Path to which model weights and results are written.\n",
    "    @param batch_size (int): Number of examples in a single batch\n",
    "    @param n_epochs (int): Number of training epochs\n",
    "    @param lr (float): Learning rate\n",
    "    \"\"\"\n",
    "    best_dev_UAS = 0\n",
    "\n",
    "\n",
    "    ### YOUR CODE HERE (~2-7 lines)\n",
    "    ### TODO:\n",
    "    ###      1) Construct Adam Optimizer in variable `optimizer`\n",
    "    \n",
    "    # we will need to pass our parameters to the optimizer\n",
    "    #  pass it an iterable containining parameters we want optimized\n",
    "    # in our case this is going to take the parser model.model.param, which comes from nn.module()\n",
    "    # because our neural parser inherits from torch.nn - https://pytorch.org/docs/stable/nn.html\n",
    "    optimizer = optim.Adam(parser.model.parameters())\n",
    "    \n",
    "    ###      2) Construct the Cross Entropy Loss Function in variable `loss_func`\n",
    "    ### we end up passing this into the train_for_epoch\n",
    "    # just simply instantiation of class CEL, which we then use for output loss \n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### Hint: Use `parser.model.parameters()` to pass optimizer\n",
    "    ###       necessary parameters to tune.\n",
    "    ### Please see the following docs for support:\n",
    "    ###     Adam Optimizer: https://pytorch.org/docs/stable/optim.html\n",
    "    ###     Cross Entropy Loss: https://pytorch.org/docs/stable/nn.html#crossentropyloss\n",
    "\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"Epoch {:} out of {:}\".format(epoch + 1, n_epochs))\n",
    "        dev_UAS = train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size)\n",
    "        if dev_UAS > best_dev_UAS:\n",
    "            best_dev_UAS = dev_UAS\n",
    "            print(\"New best dev UAS! Saving model.\")\n",
    "            torch.save(parser.model.state_dict(), output_path)\n",
    "        print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size):\n",
    "    \"\"\" Train the neural dependency parser for single epoch.\n",
    "\n",
    "    Note: In PyTorch we can signify train versus test and automatically have\n",
    "    the Dropout Layer applied and removed, accordingly, by specifying\n",
    "    whether we are training, `model.train()`, or evaluating, `model.eval()`\n",
    "\n",
    "    @param parser (Parser): Neural Dependency Parser\n",
    "    @param train_data ():\n",
    "    @param dev_data ():\n",
    "    @param optimizer (nn.Optimizer): Adam Optimizer\n",
    "    @param loss_func (nn.CrossEntropyLoss): Cross Entropy Loss Function\n",
    "    @param batch_size (int): batch size\n",
    "    @param lr (float): learning rate\n",
    "\n",
    "    @return dev_UAS (float): Unlabeled Attachment Score (UAS) for dev data\n",
    "    \"\"\"\n",
    "    parser.model.train() # Places model in \"train\" mode, i.e. apply dropout layer\n",
    "    n_minibatches = math.ceil(len(train_data) / batch_size)\n",
    "    loss_meter = AverageMeter()\n",
    "\n",
    "    with tqdm(total=(n_minibatches)) as prog:\n",
    "        for i, (train_x, train_y) in enumerate(minibatches(train_data, batch_size)):\n",
    "            \n",
    "            # this just makes sure we have fresh gradients each run....zeroes out existing grad \n",
    "            # recommended in documentation \n",
    "            optimizer.zero_grad()   # remove any baggage in the optimizer\n",
    "            loss = 0. # store loss for this batch here -> don't see the point of this given what we do below\n",
    "            train_x = torch.from_numpy(train_x).long()\n",
    "            train_y = torch.from_numpy(train_y.nonzero()[1]).long()\n",
    "\n",
    "            ### YOUR CODE HERE (~5-10 lines)\n",
    "            ### TODO:\n",
    "            ###      1) Run train_x forward through model to produce `logits`\n",
    "            \n",
    "            logits = parser.model.forward(train_x)\n",
    "            \n",
    "            ###      2) Use the `loss_func` parameter to apply the PyTorch CrossEntropyLoss function.\n",
    "            ###         This will take `logits` and `train_y` as inputs. It will output the CrossEntropyLoss\n",
    "            ###         between softmax(`logits`) and `train_y`. Remember that softmax(`logits`)\n",
    "            ###         are the predictions (y^ from the PDF).\n",
    "            \n",
    "            # loss_func is just nn.CrossEntropyLoss: https://pytorch.org/docs/stable/nn.html#crossentropyloss\n",
    "            # output = loss(input, target)\n",
    "            # output.backward()\n",
    "            loss = loss_func(logits, train_y)\n",
    "            \n",
    "            \n",
    "            ###      3) Backprop losses - running \n",
    "            loss.backward()\n",
    "            \n",
    "            ###      4) Take step with the optimizer\n",
    "            ### Please see the following docs for support:\n",
    "            ###     Optimizer Step: https://pytorch.org/docs/stable/optim.html#optimizer-step\n",
    "            \n",
    "            # Assumption: We pass in an instance of Optimizer which stores our weights....\n",
    "            # I think i will build optimizer in earlier functions\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "\n",
    "            ### END YOUR CODE\n",
    "            prog.update(1)\n",
    "            loss_meter.update(loss.item())\n",
    "\n",
    "    print (\"Average Train Loss: {}\".format(loss_meter.avg))\n",
    "\n",
    "    print(\"Evaluating on dev set\",)\n",
    "    parser.model.eval() # Places model in \"eval\" mode, i.e. don't apply dropout layer\n",
    "    dev_UAS, _ = parser.parse(dev_data)\n",
    "    print(\"- dev UAS: {:.2f}\".format(dev_UAS * 100.0))\n",
    "    return dev_UAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the full process:\n",
    "\n",
    "- This should just output a very small set.\n",
    "\n",
    "- From assignment: \n",
    "    - When running with debug=True, you should be able to get a loss smaller than 0.2 and a UAS\n",
    "larger than 65 on the dev set (although in rare cases your results may be lower, there is some\n",
    "randomness when training).\n",
    "   - It should take about 1 hour to train the model on the entire the training dataset, i.e., when\n",
    "debug=False.\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INITIALIZING\n",
      "================================================================================\n",
      "Loading data...\n",
      "took 2.95 seconds\n",
      "Building parser...\n",
      "took 1.50 seconds\n",
      "Loading pretrained embeddings...\n",
      "took 3.59 seconds\n",
      "Vectorizing data...\n",
      "took 2.06 seconds\n",
      "Preprocessing training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1848 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 59.45 seconds\n",
      "took 0.03 seconds\n",
      "\n",
      "================================================================================\n",
      "TRAINING\n",
      "================================================================================\n",
      "Epoch 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1848/1848 [04:44<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.17285270159620614\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1445850it [00:00, 8026607.10it/s]       \n",
      "  0%|          | 0/1848 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 84.56\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1848/1848 [04:42<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.11140619965826536\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1445850it [00:00, 24322229.46it/s]      \n",
      "  0%|          | 0/1848 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 86.65\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 3 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1848/1848 [04:41<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.09637250826448138\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1445850it [00:00, 42238388.83it/s]      \n",
      "  0%|          | 0/1848 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 87.29\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 4 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1848/1848 [04:41<00:00,  6.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.08726071365161504\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1445850it [00:00, 35639641.26it/s]      \n",
      "  0%|          | 0/1848 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 87.49\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 5 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1848/1848 [04:42<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.08039719181022409\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1445850it [00:00, 28147032.46it/s]      \n",
      "  0%|          | 0/1848 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 87.99\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 6 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1848/1848 [04:39<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.07483406893675139\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1445850it [00:00, 40325930.71it/s]      \n",
      "  0%|          | 0/1848 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 87.99\n",
      "\n",
      "Epoch 7 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1848/1848 [04:40<00:00,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.07007720324440629\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1445850it [00:00, 29419522.53it/s]      \n",
      "  0%|          | 0/1848 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 87.98\n",
      "\n",
      "Epoch 8 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1848/1848 [04:41<00:00,  6.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.06603788080306731\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1445850it [00:00, 28442877.89it/s]      \n",
      "  0%|          | 0/1848 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 88.19\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 9 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1848/1848 [04:40<00:00,  6.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.06218883276663043\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1445850it [00:00, 24192115.81it/s]      \n",
      "  0%|          | 0/1848 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 88.32\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 10 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1848/1848 [04:46<00:00,  6.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.05876554490280745\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1445850it [00:00, 45239684.28it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 87.95\n",
      "\n",
      "================================================================================\n",
      "TESTING\n",
      "================================================================================\n",
      "Restoring the best model weights found on the dev set\n",
      "Final evaluation on test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2919736it [00:00, 39760843.85it/s]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- test UAS: 88.82\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "debug = False\n",
    "\n",
    "# hope this is not an issue\n",
    "#assert(torch.__version__ == \"1.0.0\"),  \"Please install torch version 1.0.0\"\n",
    "\n",
    "print(80 * \"=\")\n",
    "print(\"INITIALIZING\")\n",
    "print(80 * \"=\")\n",
    "parser, embeddings, train_data, dev_data, test_data = load_and_preprocess_data(debug)\n",
    "\n",
    "start = time.time()\n",
    "model = ParserModel(embeddings)\n",
    "parser.model = model\n",
    "print(\"took {:.2f} seconds\\n\".format(time.time() - start))\n",
    "\n",
    "print(80 * \"=\")\n",
    "print(\"TRAINING\")\n",
    "print(80 * \"=\")\n",
    "output_dir = \"results/{:%Y%m%d_%H%M%S}/\".format(datetime.now())\n",
    "output_path = output_dir + \"model.weights\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005)\n",
    "\n",
    "if not debug:\n",
    "    print(80 * \"=\")\n",
    "    print(\"TESTING\")\n",
    "    print(80 * \"=\")\n",
    "    print(\"Restoring the best model weights found on the dev set\")\n",
    "    parser.model.load_state_dict(torch.load(output_path))\n",
    "    print(\"Final evaluation on test set\",)\n",
    "    parser.model.eval()\n",
    "    UAS, dependencies = parser.parse(test_data)\n",
    "    print(\"- test UAS: {:.2f}\".format(UAS * 100.0))\n",
    "    print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
