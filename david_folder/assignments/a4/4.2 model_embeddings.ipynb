{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEmbeddings(nn.Module): \n",
    "    \"\"\"\n",
    "    Class that converts input words to their embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size, vocab):\n",
    "        \"\"\"\n",
    "        Init the Embedding layers.\n",
    "\n",
    "        @param embed_size (int): Embedding size (dimensionality)\n",
    "        @param vocab (Vocab): Vocabulary object containing src and tgt languages\n",
    "                              See vocab.py for documentation.\n",
    "        \"\"\"\n",
    "        super(ModelEmbeddings, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        # default values\n",
    "        self.source = None\n",
    "        self.target = None\n",
    "\n",
    "        src_pad_token_idx = vocab.src['<pad>']\n",
    "        tgt_pad_token_idx = vocab.tgt['<pad>']\n",
    "\n",
    "        ### YOUR CODE HERE (~2 Lines)\n",
    "        ### TODO - Initialize the following variables:\n",
    "        ###     self.source (Embedding Layer for source language)\n",
    "        ###     self.target (Embedding Layer for target langauge)\n",
    "        ###\n",
    "        ### Note:\n",
    "        ###     1. `vocab` object contains two vocabularies:\n",
    "        ###            `vocab.src` for source\n",
    "        ###            `vocab.tgt` for target\n",
    "        ###     2. You can get the length of a specific vocabulary by running:\n",
    "        ###             `len(vocab.<specific_vocabulary>)`\n",
    "        ###     3. Remember to include the padding token for the specific vocabulary\n",
    "        ###        when creating your Embedding.\n",
    "        ###\n",
    "        ### Use the following docs to properly initialize these variables:\n",
    "        ###     Embedding Layer:\n",
    "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding\n",
    "        \n",
    "        # we start with source language embeddings\n",
    "        # each word in a sentence gets an embedding of size e (embed_size); num of embeddings is just the number of words in vocab\n",
    "        self.source = nn.Embedding(num_embeddings= len(vocab.src), embedding_dim = embed_size, padding_idx = src_pad_token_idx)\n",
    "        \n",
    "        # target language embeddings will be similar, just different vocab and padding\n",
    "        self.target = nn.Embedding(num_embeddings= len(vocab.tgt), embedding_dim = embed_size, padding_idx = tgt_pad_token_idx)\n",
    "\n",
    "        ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
